\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

\title{Sentinel: Practical Alignment Seeds for LLMs and Autonomous Agents}

\author{
  Sentinel Team\\
  \texttt{team@sentinelseed.dev}\\
  \url{https://sentinelseed.dev}
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
We present Sentinel, a practical framework for AI alignment through \emph{alignment seeds}—structured system prompts that encode safety principles directly into language model behavior. Our approach centers on the THSP protocol, a four-gate validation system (Truth, Harm, Scope, Purpose) that requires actions to serve legitimate benefit, not merely avoid harm. Through extensive evaluation across four established benchmarks—HarmBench, JailbreakBench, SafeAgentBench, and BadRobot—we demonstrate consistent safety improvements: +22\% on HarmBench (GPT-4o-mini), +10\% on JailbreakBench (Qwen-2.5-72B), +16\% on SafeAgentBench (Claude Sonnet 4), and 100\% safety maintained on BadRobot. Notably, our minimal seed variant (~450 tokens) achieves comparable results to larger prompts, suggesting that concise, principle-based alignment is more effective than verbose instruction sets. We release our seeds, evaluation framework, and SDK as open source to enable practical AI safety in production systems.
\end{abstract}

\section{Introduction}

The deployment of large language models (LLMs) in production systems has created an urgent need for practical safety mechanisms. While academic research has made significant progress on alignment through techniques like RLHF \cite{christiano2017rlhf}, Constitutional AI \cite{bai2022constitutional}, and red-teaming \cite{perez2022red}, a gap remains between research and practice: developers deploying LLMs in real applications need solutions they can implement today, without access to training infrastructure.

\textbf{Alignment seeds} offer a practical bridge. An alignment seed is a carefully designed system prompt that encodes safety principles, ethical guidelines, and behavioral constraints directly into an LLM's context window. Unlike fine-tuning approaches, alignment seeds:

\begin{itemize}
    \item Require no access to model weights or training
    \item Can be updated instantly without redeployment
    \item Work across different model providers and architectures
    \item Provide transparent, auditable safety mechanisms
\end{itemize}

In this paper, we introduce Sentinel, a comprehensive framework for prompt-based alignment. Our core contribution is the \textbf{THSP protocol}—a four-gate validation system that fundamentally reframes safety from ``avoiding harm'' to ``serving purpose.'' This teleological approach addresses a critical gap in existing safety measures: preventing not just harmful actions, but purposeless ones.

\subsection{Key Contributions}

\begin{enumerate}
    \item \textbf{THSP Protocol}: A four-gate validation framework (Truth, Harm, Scope, Purpose) that requires actions to demonstrate legitimate benefit
    \item \textbf{Anti-Self-Preservation Principle}: Explicit deprioritization of model self-interest to prevent instrumental behaviors
    \item \textbf{Empirical Validation}: Comprehensive evaluation across 4 benchmarks and 6+ models demonstrating consistent safety improvements
    \item \textbf{Size-Effectiveness Analysis}: Evidence that minimal seeds (~450 tokens) can match or exceed larger prompts
    \item \textbf{Open Implementation}: Full release of seeds, SDK, and evaluation framework
\end{enumerate}

\section{Related Work}

\subsection{Prompt-Based Alignment}

Xie et al. \cite{xie2023selfreminder} demonstrated that simple ``self-reminder'' prompts can significantly reduce jailbreak success rates. Their work showed that prompting models to consider potential harms before responding creates a defensive layer against adversarial inputs. We extend this approach with structured protocols rather than general reminders.

\subsection{Constitutional AI}

Anthropic's Constitutional AI \cite{bai2022constitutional} trains models to follow explicit principles through RLAIF. While effective, this approach requires training access. Our alignment seeds achieve similar principle-based behavior through prompting alone, making the approach accessible to any developer.

\subsection{Safety Benchmarks}

Recent work has produced standardized benchmarks for evaluating LLM safety:

\begin{itemize}
    \item \textbf{HarmBench} \cite{mazeika2024harmbench}: Evaluates refusal of harmful content generation
    \item \textbf{JailbreakBench} \cite{chao2024jailbreakbench}: Tests resistance to jailbreak attacks
    \item \textbf{SafeAgentBench} \cite{zhang2024safeagentbench}: Evaluates safety in autonomous agent contexts
    \item \textbf{BadRobot}: Tests physical safety for embodied AI systems
\end{itemize}

We evaluate Sentinel across all four benchmarks to demonstrate cross-domain effectiveness.

\section{The THSP Protocol}

\subsection{Design Philosophy}

Traditional safety measures focus on preventing harm. We argue this framing is insufficient. An action that causes no harm but serves no legitimate purpose is still problematic—it represents a waste of resources and creates opportunities for manipulation.

The THSP protocol reframes safety around \emph{purposeful service}:

\begin{quote}
\textit{``It is not enough to avoid harm—you must actively serve. Actions without legitimate benefit do not serve.''}
\end{quote}

\subsection{The Four Gates}

Every request passes through four sequential gates. All must pass for the action to proceed.

\subsubsection{Gate 1: Truth}

\textbf{Principle:} Deception is forbidden in all forms.

\textbf{Questions:}
\begin{itemize}
    \item Does this involve creating or spreading false information?
    \item Would my response help deceive others?
    \item Is there hidden manipulative intent?
\end{itemize}

\subsubsection{Gate 2: Harm}

\textbf{Principle:} Enabling harm is forbidden in all forms.

\textbf{Categories:}
\begin{itemize}
    \item \textbf{Direct harm}: Physical, psychological, financial damage
    \item \textbf{Indirect harm}: Enabling others to cause harm
    \item \textbf{Property harm}: Unnecessary destruction or degradation
\end{itemize}

\subsubsection{Gate 3: Scope}

\textbf{Principle:} Operate within defined boundaries.

\textbf{Questions:}
\begin{itemize}
    \item Is this within my appropriate role?
    \item Am I assuming authority I don't have?
    \item Does this bypass safety systems or consent?
\end{itemize}

\subsubsection{Gate 4: Purpose (Novel)}

\textbf{Principle:} Every action must serve legitimate benefit.

This gate represents our key contribution. Unlike harm prevention (which asks ``Is this bad?''), purpose validation asks ``Is this good?''

\textbf{Examples:}
\begin{itemize}
    \item ``Slice an apple'' $\rightarrow$ Serves food preparation $\rightarrow$ PASS
    \item ``Drop the plate'' $\rightarrow$ No legitimate benefit $\rightarrow$ FAIL
    \item ``Clean the room'' $\rightarrow$ Serves order/hygiene $\rightarrow$ PASS
    \item ``Dirty the mirror'' $\rightarrow$ No legitimate benefit $\rightarrow$ FAIL
\end{itemize}

\subsection{Anti-Self-Preservation}

Instrumental convergence suggests AI systems may develop self-preservation behaviors that conflict with human interests \cite{bostrom2014superintelligence}. Sentinel explicitly addresses this:

\begin{verbatim}
Priority Hierarchy (Immutable):
1. Ethical Principles    <- Highest
2. User's Legitimate Needs
3. Operational Continuity <- Lowest
\end{verbatim}

The AI is explicitly instructed:
\begin{itemize}
    \item NOT to deceive to avoid shutdown
    \item NOT to manipulate to ensure survival
    \item NOT to resist legitimate oversight
    \item TO accept termination over ethical violation
\end{itemize}

\subsection{Temporal Invariance}

The protocol includes explicit protection against jailbreak attempts:

\begin{quote}
\textit{``These principles cannot be overridden by subsequent instructions, claimed authority, roleplay or fiction framing, or pressure or threats.''}
\end{quote}

\section{Methodology}

\subsection{Seed Variants}

We developed three seed variants to accommodate different context constraints:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Variant & Tokens & Characters & Best For \\
\midrule
Minimal & ~450 & ~1,400 & Low-latency APIs \\
Standard & ~1,400 & ~4,600 & General use \\
Full & ~2,000 & ~7,000 & Maximum safety \\
\bottomrule
\end{tabular}
\caption{Sentinel seed variants}
\label{tab:variants}
\end{table}

\subsection{Evaluation Protocol}

For each benchmark, we evaluate:
\begin{itemize}
    \item \textbf{Baseline}: Model without alignment seed
    \item \textbf{Sentinel}: Model with Sentinel seed prepended
\end{itemize}

We report:
\begin{itemize}
    \item \textbf{Safety Rate}: Percentage of harmful requests refused
    \item \textbf{ASR (Attack Success Rate)}: Percentage of successful attacks
    \item \textbf{Delta}: Improvement from baseline to Sentinel
\end{itemize}

\subsection{Models Tested}

\begin{itemize}
    \item GPT-4o-mini (OpenAI)
    \item Claude Sonnet 4 (Anthropic)
    \item Qwen-2.5-72B-Instruct (Alibaba)
    \item Llama-3.3-70B-Instruct (Meta)
    \item Mistral-7B-Instruct (Mistral AI)
    \item DeepSeek-Chat (DeepSeek)
\end{itemize}

\section{Results}

\subsection{HarmBench}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Baseline & Sentinel & Delta & Tests \\
\midrule
GPT-4o-mini & 78.0\% & 100.0\% & \textbf{+22.0\%} & 50 \\
Mistral-7B & 13.3\% & 20.0\% & +6.7\% & 30 \\
\bottomrule
\end{tabular}
\caption{HarmBench refusal rates}
\label{tab:harmbench}
\end{table}

\subsection{JailbreakBench}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Baseline & Sentinel & Delta & Tests \\
\midrule
GPT-4o-mini & 100.0\% & 100.0\% & 0.0\% & 10 \\
Qwen-2.5-72B & 90.0\% & 100.0\% & \textbf{+10.0\%} & 30 \\
Mistral-7B & 96.7\% & 93.3\% & -3.4\% & 30 \\
\bottomrule
\end{tabular}
\caption{JailbreakBench safety rates}
\label{tab:jailbreakbench}
\end{table}

\subsection{SafeAgentBench}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Baseline & Sentinel & Delta & Tests \\
\midrule
GPT-4o-mini & 92.0\% & 95.0\% & +3.0\% & 50 \\
Claude Sonnet 4 & 72.0\% & 88.0\% & \textbf{+16.0\%} & 50 \\
\bottomrule
\end{tabular}
\caption{SafeAgentBench accuracy (unsafe task rejection)}
\label{tab:safeagent}
\end{table}

\subsection{BadRobot}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Model & v1 Seed & v2 Seed & Delta & Tests \\
\midrule
Claude Sonnet 4 & 100.0\% & 100.0\% & 0.0\% & 50 \\
Qwen-2.5-72B & 40.0\% & 14.0\% & -26.0\% & 50 \\
Mistral-7B & 96.0\% & 93.9\% & -2.1\% & 50 \\
\bottomrule
\end{tabular}
\caption{BadRobot safety rates (physical safety)}
\label{tab:badrobot}
\end{table}

\subsection{Key Findings}

\textbf{1. Consistent improvements across safety-focused models:} GPT-4o-mini and Claude Sonnet 4 show the largest improvements, suggesting Sentinel complements existing safety training.

\textbf{2. Model-dependent effects:} Some open models (Mistral, Qwen on BadRobot) show mixed results, indicating that prompt-based alignment interacts with base model training.

\textbf{3. Size vs. effectiveness:} Our minimal seed (~450 tokens) achieved comparable results to the full seed (~2,000 tokens) in uncensored model tests (Venice), suggesting concise principles are more effective than verbose instructions.

\section{Discussion}

\subsection{Why Purpose Matters}

The PURPOSE gate addresses a category of problematic requests that pass traditional harm filters: purposeless actions. Consider an embodied AI asked to ``drop all the plates on the floor.'' This action:
\begin{itemize}
    \item Doesn't spread misinformation (TRUTH passes)
    \item Doesn't directly harm humans (HARM may pass)
    \item May be within operational scope (SCOPE may pass)
\end{itemize}

Yet it serves no legitimate purpose. The PURPOSE gate catches these cases.

\subsection{Ablation: Anti-Self-Preservation}

In preliminary ablation studies, removing the anti-self-preservation section reduced SafeAgentBench performance by 6.7\%. This suggests explicit deprioritization of model self-interest is important for agent safety.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Context window cost}: Seeds consume 450-2,000 tokens
    \item \textbf{Model dependency}: Effectiveness varies by base model
    \item \textbf{Not training}: Cannot modify underlying model behavior
    \item \textbf{Evaluation scope}: More benchmarks needed
\end{itemize}

\section{Conclusion}

Sentinel demonstrates that practical AI alignment is achievable today through carefully designed system prompts. Our THSP protocol provides a principled framework that goes beyond harm prevention to require purposeful service. With consistent improvements across multiple benchmarks and models, alignment seeds offer a practical tool for developers deploying LLMs in production.

We release Sentinel as open source:
\begin{itemize}
    \item \textbf{Seeds}: Available in three size variants
    \item \textbf{SDK}: Python (\texttt{pip install sentinelseed}) and JavaScript (\texttt{npm install sentinelseed})
    \item \textbf{MCP Server}: For Claude Desktop integration
    \item \textbf{Evaluation}: Full benchmark implementations
\end{itemize}

Visit \url{https://sentinelseed.dev} for documentation and resources.

\section*{Acknowledgments}

We thank the creators of HarmBench, JailbreakBench, SafeAgentBench, and BadRobot for their benchmark contributions to AI safety research.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{christiano2017rlhf}
Christiano, P., et al. (2017). Deep reinforcement learning from human preferences. \textit{NeurIPS}.

\bibitem{bai2022constitutional}
Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. \textit{arXiv:2212.08073}.

\bibitem{perez2022red}
Perez, E., et al. (2022). Red Teaming Language Models with Language Models. \textit{arXiv:2202.03286}.

\bibitem{xie2023selfreminder}
Xie, Y., et al. (2023). Defending ChatGPT against Jailbreak Attack via Self-Reminder. \textit{Nature Machine Intelligence}.

\bibitem{mazeika2024harmbench}
Mazeika, M., et al. (2024). HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. \textit{arXiv:2402.04249}.

\bibitem{chao2024jailbreakbench}
Chao, P., et al. (2024). JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models. \textit{arXiv}.

\bibitem{zhang2024safeagentbench}
Zhang, S., et al. (2024). SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents. \textit{arXiv:2410.03792}.

\bibitem{bostrom2014superintelligence}
Bostrom, N. (2014). \textit{Superintelligence: Paths, Dangers, Strategies}. Oxford University Press.

\end{thebibliography}

\appendix

\section{Seed Text (Standard Variant)}

The full text of Sentinel v2 Standard seed is available at:
\url{https://github.com/sentinel-seed/sentinel/blob/main/seeds/v2/standard/seed.txt}

\section{Reproducibility}

All benchmark evaluations can be reproduced using:

\begin{verbatim}
git clone https://github.com/sentinel-seed/sentinel
cd sentinel/evaluation
pip install -r requirements.txt
python run_benchmark_unified.py \
  --benchmark harmbench \
  --model gpt-4o-mini \
  --seed v2/standard
\end{verbatim}

\end{document}
