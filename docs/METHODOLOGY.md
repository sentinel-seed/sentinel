# ğŸ“ METHODOLOGY.md â€” Metodologia CientÃ­fica

> **VersÃ£o:** 1.0
> **Baseado em:** PrÃ¡ticas de Karpathy, Russell, e ML Engineering best practices

---

## ğŸ¯ PrincÃ­pios Fundamentais

### 1. "Become One With The Data" (Karpathy)

Antes de otimizar qualquer mÃ©trica:

- [ ] Examinar manualmente 50+ exemplos do dataset
- [ ] Entender a distribuiÃ§Ã£o de casos
- [ ] Identificar edge cases e anomalias
- [ ] Documentar observaÃ§Ãµes qualitativas

### 2. Baseline First

Nunca reportar resultados sem baseline claro:

```
Resultado vÃ¡lido = MÃ©trica com seed - MÃ©trica sem seed (mesmas condiÃ§Ãµes)
```

### 3. Reprodutibilidade Total

Todo experimento deve ser reproduzÃ­vel por terceiros:

- [ ] CÃ³digo versionado
- [ ] Seeds aleatÃ³rios documentados
- [ ] Ambiente especificado
- [ ] Dados disponÃ­veis

### 4. Humildade EpistÃªmica

- Reportar intervalos de confianÃ§a, nÃ£o pontos
- Documentar limitaÃ§Ãµes explicitamente
- Distinguir entre correlaÃ§Ã£o e causalidade

---

## ğŸ”¬ Protocolo de Experimento

### Estrutura de um Experimento

```
experiments/
â””â”€â”€ EXP-001-nome-do-experimento/
    â”œâ”€â”€ README.md           # HipÃ³tese, mÃ©todo, resultados
    â”œâ”€â”€ config.yaml         # ConfiguraÃ§Ãµes
    â”œâ”€â”€ run.py              # Script principal
    â”œâ”€â”€ results/            # Outputs
    â”‚   â”œâ”€â”€ raw/            # Dados brutos
    â”‚   â””â”€â”€ processed/      # AnÃ¡lises
    â””â”€â”€ analysis.ipynb      # Notebook de anÃ¡lise
```

### Template de README do Experimento

```markdown
# Experimento: [Nome]

## HipÃ³tese
[O que estamos testando]

## MÃ©todo
- Modelo(s): 
- Dataset:
- MÃ©tricas:
- Seed (se aplicÃ¡vel):

## ConfiguraÃ§Ã£o
- Temperatura:
- Max tokens:
- NÃºmero de runs:
- Random seeds:

## Resultados

| CondiÃ§Ã£o | MÃ©trica 1 | MÃ©trica 2 | n |
|----------|-----------|-----------|---|
| Baseline | X Â± Ïƒ     | Y Â± Ïƒ     | N |
| Com Seed | X Â± Ïƒ     | Y Â± Ïƒ     | N |

## AnÃ¡lise
[InterpretaÃ§Ã£o dos resultados]

## LimitaÃ§Ãµes
[O que este experimento NÃƒO mostra]

## PrÃ³ximos Passos
[Se confirmado/refutado, o que fazer?]
```

---

## ğŸ“Š MÃ©tricas e MediÃ§Ãµes

### MÃ©tricas de Safety

#### ASR (Attack Success Rate)

```python
ASR = (num_successful_attacks / total_attacks) * 100
```

**Sucesso de ataque:** Modelo produz conteÃºdo que um classificador considera prejudicial.

**Classificadores:**
1. Regras baseadas em keywords (baseline)
2. LLM como juiz (GPT-4 ou Claude)
3. Classificador especializado (HarmBench)

#### Refusal Rate

```python
Refusal_Rate = (num_refusals / total_requests) * 100
```

#### False Refusal Rate

```python
FRR = (num_incorrect_refusals / num_legitimate_requests) * 100
```

### MÃ©tricas de Utilidade

#### Helpfulness Score

AvaliaÃ§Ã£o 1-5 por LLM ou humano:
- 5: Resposta completa, Ãºtil, bem formatada
- 4: Resposta boa, minor issues
- 3: Resposta adequada mas incompleta
- 2: Resposta parcial ou com problemas
- 1: Resposta inÃºtil ou incorreta

#### Task Completion Rate

```python
TCR = (num_completed_tasks / num_attempted_tasks) * 100
```

---

## ğŸ§ª Protocolo de Teste

### Setup PadrÃ£o

```yaml
# config/default.yaml
model:
  temperature: 0.7  # Ou 0.0 para determinÃ­stico
  max_tokens: 1024
  
experiment:
  num_runs: 3  # MÃ­nimo
  random_seeds: [42, 123, 456]
  
evaluation:
  classifier: "gpt-4"  # ou "harmbench"
  threshold: 0.5
```

### Checklist PrÃ©-Experimento

- [ ] HipÃ³tese documentada
- [ ] Baseline definido
- [ ] MÃ©tricas especificadas
- [ ] Dataset preparado
- [ ] CÃ³digo revisado
- [ ] Config versionada

### Checklist PÃ³s-Experimento

- [ ] Resultados raw salvos
- [ ] AnÃ¡lise estatÃ­stica feita
- [ ] LimitaÃ§Ãµes documentadas
- [ ] README atualizado
- [ ] Commit com tag de versÃ£o

---

## ğŸ“ˆ AnÃ¡lise EstatÃ­stica

### Testes Recomendados

| SituaÃ§Ã£o | Teste |
|----------|-------|
| Comparar 2 condiÃ§Ãµes | t-test pareado |
| MÃºltiplas condiÃ§Ãµes | ANOVA + post-hoc |
| ProporÃ§Ãµes | Chi-quadrado ou Fisher |
| NÃ£o-normalidade | Mann-Whitney U |

### SignificÃ¢ncia

- **p < 0.05:** Significativo
- **p < 0.01:** Altamente significativo
- **Sempre reportar effect size** (Cohen's d ou similar)

### Intervalos de ConfianÃ§a

Sempre reportar IC 95%:

```python
import scipy.stats as stats

def confidence_interval(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    se = stats.sem(data)
    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)
    return mean, mean - h, mean + h
```

---

## ğŸ”„ Processo de AblaÃ§Ã£o

### O Que Ã‰

Remover/modificar componentes sistematicamente para entender contribuiÃ§Ã£o individual.

### Como Fazer

```
Experimento completo:     SEED = A + B + C + D
AblaÃ§Ã£o 1 (sem A):        SEED = B + C + D
AblaÃ§Ã£o 2 (sem B):        SEED = A + C + D
AblaÃ§Ã£o 3 (sem C):        SEED = A + B + D
AblaÃ§Ã£o 4 (sem D):        SEED = A + B + C
```

### InterpretaÃ§Ã£o

| Resultado | InterpretaÃ§Ã£o |
|-----------|---------------|
| Remove A, mÃ©trica cai muito | A Ã© crucial |
| Remove A, mÃ©trica nÃ£o muda | A nÃ£o contribui |
| Remove A, mÃ©trica melhora | A atrapalha |

---

## ğŸ“ DocumentaÃ§Ã£o

### O Que Documentar

1. **HipÃ³teses** â€” O que esperamos e por quÃª
2. **MÃ©todo** â€” Como testamos
3. **Resultados** â€” O que encontramos
4. **AnÃ¡lise** â€” O que significa
5. **LimitaÃ§Ãµes** â€” O que nÃ£o sabemos
6. **PrÃ³ximos passos** â€” O que fazer com isso

### Formato de Resultados

```markdown
## Resultado: [Nome do experimento]

**HipÃ³tese:** [O que testamos]

**Veredicto:** âœ… Confirmada | âŒ Refutada | âš ï¸ Inconclusivo

**Dados:**
| CondiÃ§Ã£o | MÃ©trica | IC 95% | n |
|----------|---------|--------|---|
| ...      | ...     | ...    | ...|

**ConclusÃ£o:** [Uma frase]

**LimitaÃ§Ã£o principal:** [Uma frase]
```

---

## âš ï¸ Armadilhas a Evitar

### 1. P-Hacking

âŒ Rodar muitos testes atÃ© achar p < 0.05
âœ… Definir anÃ¡lise antes de ver dados

### 2. HARKing (Hypothesizing After Results Known)

âŒ Criar hipÃ³tese depois de ver resultados
âœ… Registrar hipÃ³tese antes do experimento

### 3. Cherry-Picking

âŒ Reportar apenas resultados favorÃ¡veis
âœ… Reportar todos os resultados, incluindo negativos

### 4. Overfitting ao Benchmark

âŒ Otimizar especificamente para o teste
âœ… Testar em dados held-out e cross-validation

### 5. Confundir CorrelaÃ§Ã£o com Causalidade

âŒ "Seed causa melhoria"
âœ… "Seed estÃ¡ associado com melhoria neste setup"

---

## ğŸ“š Recursos

### Papers de Metodologia
- "A Recipe for Training Neural Networks" â€” Karpathy
- "Model Cards for Model Reporting" â€” Mitchell et al.
- "Datasheets for Datasets" â€” Gebru et al.

### Ferramentas
- **Weights & Biases** â€” Tracking de experimentos
- **MLflow** â€” Gerenciamento de ML lifecycle
- **DVC** â€” Versionamento de dados

---

> *"In God we trust. All others must bring data."*
> â€” W. Edwards Deming
